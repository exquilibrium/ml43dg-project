{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\n",
    "\n",
    "**Presentation**: Wednesday, July 24th 16:54\n",
    "\n",
    "**Report and Code**: August 14th, 23:55\n",
    "\n",
    "Dataset:\n",
    "- Objectverse (https://objaverse.allenai.org)\n",
    "\n",
    "Modifications:\n",
    "- Explore various ways of improving the generalization ability across different categories, e.g., adding class embedding, and text feature descriptors.\n",
    "- Adding additional test-time optimization loss for better shape fitting especially for objects with thin structure\n",
    "- Add color code to shape latent code, also learn view-independant color field given colored-mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:04:20.689329400Z",
     "start_time": "2024-07-23T18:04:02.831691900Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet objaverse\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import objaverse\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DeepSDF\n",
    "\n",
    "\n",
    "Here, we will take a look at 3D-reconstruction using [DeepSDF](https://arxiv.org/abs/1901.05103). We recommend reading the paper before attempting the exercise.\n",
    "\n",
    "DeepSDF is an auto-decoder based approach that learns a continuous SDF representation for a class of shapes. Once trained, it can be used for shape representation, interpolation and shape completion. We'll look at each of these\n",
    "applications.\n",
    "\n",
    "<img src=\"ml43dg/images/deepsdf_teaser.png\" alt=\"deepsdf_teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "During training, the autodecoder optimizes both the network parameters and the latent codes representing each of the training shapes. Once trained, to reconstruct a shape given its SDF observations, a latent code is\n",
    "optimized keeping the network parameters fixed, such that the optimized latent code gives the lowest error with observed SDF values.\n",
    "\n",
    "An advantage that implicit representations have over voxel/grid based approaches is that they are not tied to a particular grid resolution, and can be evaluated at any resolution once trained.\n",
    "\n",
    "Similar to previous exercise, we'll first download the processed dataset, look at the implementation of the dataset, the model and the trainer, try out overfitting and generalization over the entire dataset, and finally inference on unseen samples.\n",
    "\n",
    "### (a) Downloading the data\n",
    "\n",
    "Whereas volumetric models output entire 3d shape representations, implicit models like DeepSDF work on per point basis. The network takes in a 3D-coordinate (and additionally the latent vector) and outputs the SDF value at the queried point. To train such a model,\n",
    "we therefore need, for each of the training shapes, a bunch of points with their corresponding SDF values for supervision. Points are sampled more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For those curious,\n",
    "data preparation is decribed in more detail in section 5 of the paper.\n",
    "\n",
    "We'll be using the Objaverse Chairs class for the experiments in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path(\"./ml43dg/data/objaverse/\")\n",
    "\n",
    "for class_name in ['sofa', 'table', 'vase']:\n",
    "    print(\"Downloading \" + (class_name+\"s\"))\n",
    "    lvis_annotations = objaverse.load_lvis_annotations()\n",
    "    random.seed(42)\n",
    "    uids = lvis_annotations[class_name]\n",
    "    uids = random.sample(uids, 5)\n",
    "\n",
    "    objects = objaverse.load_objects(\n",
    "        uids=uids,\n",
    "    )\n",
    "\n",
    "    for objaverse_id, file_path in objects.items():\n",
    "        if not Path(download_path / (class_name+\"s\") / objaverse_id).exists():\n",
    "            Path(download_path / (class_name+\"s\")).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.move(file_path, download_path / (class_name+\"s\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shape, the downloaded chair .glb will be converted into a coloured mesh and stored in the same directory with its corresponding sdf file:\n",
    "- `mesh.obj` representing the mesh representation of the shape\n",
    "- `sdf.npz` file containing large number of points sampled on and around the mesh and their sdf values; contains numpy arrays under keys \"pos\" and \"neg\", containing points with positive and negative sdf values respectively\n",
    "\n",
    "```\n",
    "# contents of ml43dg/data/objaverse\n",
    "1faa4c299b93a3e5593ebeeedbff73b/                    # shape 0\n",
    "    ├── mesh.obj                                    # shape 0 mesh\n",
    "    ├── sdf.npz                                     # shape 0 sdf\n",
    "    ├── surface.obj                                 # shape 0 surface\n",
    "1fde48d83065ef5877a929f61fea4d0/                    # shape 1\n",
    "1fe1411b6c8097acf008d8a3590fb522/                   # shape 2\n",
    ":\n",
    "```\n",
    "The processing is performed in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"tables\", \"sofas\", \"chairs\", \"vases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml43dg.data.preprocessing import preprocess\n",
    "\n",
    "class_directories = {}\n",
    "for class_name in classes:\n",
    "    files = os.listdir(\"./ml43dg/data/objaverse/\"+class_name)\n",
    "    class_directories[class_name] = []\n",
    "    for file in files:\n",
    "        filename = Path(file).stem\n",
    "        class_directories[class_name].append(filename)\n",
    "\n",
    "for class_name in class_directories:\n",
    "    random.seed(42)\n",
    "    random.shuffle(class_directories[class_name])\n",
    "    chunk = {class_name: class_directories[class_name][:80]}\n",
    "    print(chunk)\n",
    "    preprocess(data_dir=\"./ml43dg/data/preprocessed/\", source_dir=\"./ml43dg/data/objaverse/\", source_name=\"objaverse\",  class_directories=chunk, number_of_points=200000, extension=\".obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing sdf.npz files in the objaverse directory\n",
    "for class_name in classes:\n",
    "    directories = os.listdir(\"./ml43dg/data/objaverse/\"+class_name)\n",
    "    for file in directories:\n",
    "        print(file)\n",
    "        if file.endswith(\".npz\"):\n",
    "            os.remove(\"./ml43dg/data/objaverse/\"+class_name+\"/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the preprocessed sdf.npz in each shape directory to the corresponding directory in the objaverse dataset\n",
    "for class_name in classes:\n",
    "    files = os.listdir(\"./ml43dg/data/preprocessed/objaverse/\"+class_name)\n",
    "    for file in files:\n",
    "        filename = Path(file).stem\n",
    "        shutil.copy(\"./ml43dg/data/preprocessed/objaverse/\"+class_name+\"/\"+filename+\"/sdf.npz\", \"./ml43dg/data/objaverse/\"+class_name+\"/\"+filename+\"/sdf.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "We randomly generate train/test splits and save the corresponding shape ids in .json files. Then, we can generate our train and validation loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:12:34.217485800Z",
     "start_time": "2024-07-23T18:12:34.142930600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using a 90-10 split for training and validation\n",
    "train_ratio = 90\n",
    "data_path = Path(\"./ml43dg/data/\")\n",
    "\n",
    "# List all the subdirectories in the objaverse directory\n",
    "shapes = [\"tables\", \"sofas\", \"chairs\", \"vases\"]\n",
    "\n",
    "all_shapes = []\n",
    "for shape in shapes:\n",
    "    all_shapes += list((data_path / \"preprocessed\" / \"objaverse\" / shape).iterdir())\n",
    "\n",
    "# Randomly shuffle the list\n",
    "random.shuffle(all_shapes)\n",
    "\n",
    "# Remove 1 shape to be used for overfitting\n",
    "overfit_shape = all_shapes.pop()\n",
    "\n",
    "# Split the list into training and validation sets\n",
    "train_shapes = all_shapes[:int(len(all_shapes) * (train_ratio / 100))]\n",
    "val_shapes = all_shapes[int(len(all_shapes) * (train_ratio / 100)):]\n",
    "\n",
    "# Store the names of the objects in corresponding text files\n",
    "# Use the string identified class_label/uid\n",
    "with open(data_path / \"splits\" / \"objaverse\" / \"train.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([c.parent.stem+\"/\"+c.stem for c in train_shapes]))\n",
    "\n",
    "with open(data_path / \"splits\" / \"objaverse\" / \"val.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([c.parent.stem+\"/\"+c.stem for c in val_shapes]))\n",
    "\n",
    "with open(data_path / \"splits\" / \"objaverse\" / \"overfit.txt\", \"w\") as f:\n",
    "    f.write(overfit_shape.parent.stem+\"/\"+overfit_shape.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:12:52.112225Z",
     "start_time": "2024-07-23T18:12:52.020062300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 232\n",
      "Length of val set: 26\n",
      "Length of overfit set: 1\n"
     ]
    }
   ],
   "source": [
    "from ml43dg.data.objaverse import Objaverse\n",
    "\n",
    "num_points_to_samples = 40000\n",
    "train_dataset = Objaverse(num_points_to_samples, \"train\")\n",
    "val_dataset = Objaverse(num_points_to_samples, \"val\")\n",
    "overfit_dataset = Objaverse(num_points_to_samples, \"overfit\")\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 1226\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the points sampled for a particular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:13:07.163868100Z",
     "start_time": "2024-07-23T18:13:07.037204Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\data\\objaverse.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_tensor = torch.tensor(pos_tensor[pos_idx], dtype=torch.float32)\n",
      "e:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\data\\objaverse.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  neg_tensor = torch.tensor(neg_tensor[neg_idx], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from ml43dg.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "uid = train_dataset[0]['name']\n",
    "class_name = train_dataset[0]['class_label']\n",
    "points = train_dataset[0]['points']\n",
    "sdf = train_dataset[0]['sdf']\n",
    "\n",
    "# sampled points inside the shape\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "\n",
    "# sampled points outside the shape\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\envs\\ml43dg\\Lib\\site-packages\\traittypes\\traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"uint32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66627a5fd1bd4541b08c74c9be42e130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = Objaverse.get_mesh(class_name+\"/\"+uid)\n",
    "print('Mesh')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with negative SDF (inside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e59e9f730474094bdaaa8815083f856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled points with positive SDF (outside)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf79fb1121e4bde96823db7e2b88004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Sampled points with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that more points are sampled close to the surface rather than away from the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The DeepSDF auto-decoder architecture is visualized below:\n",
    "\n",
    "<img src=\"ml43dg/images/deepsdf_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n",
    "Our modified DeepSDF auto-decoder architecture, that has an additional one-hot encoded class embedding and viewing direction and outputs the predicted colour in an additional head, is visualised below:\n",
    "\n",
    "<img src=\"ml43dg/images/modified_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:13:22.232041Z",
     "start_time": "2024-07-23T18:13:22.063142200Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name      | Type           | Params \n",
      "-----------------------------------------------\n",
      "0  | wnll1     | Linear         | 135680 \n",
      "1  | wnll2     | Linear         | 263168 \n",
      "2  | wnll3     | Linear         | 263168 \n",
      "3  | wnll4     | Linear         | 127986 \n",
      "4  | wnll5     | Linear         | 263168 \n",
      "5  | wnll6     | Linear         | 263168 \n",
      "6  | wnll7     | Linear         | 263168 \n",
      "7  | wnll8     | Linear         | 263168 \n",
      "8  | wnll5_col | Linear         | 329728 \n",
      "9  | fc        | Linear         | 513    \n",
      "10 | fc_col    | Linear         | 1539   \n",
      "11 | sigmoid   | Sigmoid        | 0      \n",
      "12 | relu      | ReLU           | 0      \n",
      "13 | dropout   | Dropout        | 0      \n",
      "14 | TOTAL     | DeepSDFDecoder | 2174454\n",
      "colour_in: torch.Size([4096, 642])\n",
      "\n",
      "Output tensor shape for sdf:  torch.Size([4096, 1])\n",
      "Output tensor shape for colour:  torch.Size([4096, 3])\n",
      "\n",
      "Number of traininable params: 2.17M\n"
     ]
    }
   ],
   "source": [
    "from ml43dg.model.deepsdf import DeepSDFDecoder\n",
    "from ml43dg.util.model import summarize_model\n",
    "\n",
    "deepsdf = DeepSDFDecoder(shape_latent_size=256, colour_latent_size=128, one_hot_size=4)\n",
    "print(summarize_model(deepsdf))\n",
    "\n",
    "# The input to the network is a concatenation of:\n",
    "#   - point coordinates (3) \n",
    "#   - viewing direction as Euler angles (2)\n",
    "#   - shape latent code (256 in this example)\n",
    "#   - colour latent code (128 in this example)\n",
    "#   - and the one-hot encoding of the class label (4).\n",
    "# Here we use a batch of 4096 points\n",
    "input_points = torch.randn(4096, 3)\n",
    "input_view = torch.randn(4096, 2)\n",
    "shape_latent = torch.randn(4096, 256)\n",
    "colour_latent = torch.randn(4096, 128)\n",
    "class_label = torch.randn(4096, 4)\n",
    "sdf_predictions, colour_predictions  = deepsdf(input_points, input_view, shape_latent, colour_latent, class_label)\n",
    "\n",
    "print('\\nOutput tensor shape for sdf: ', sdf_predictions.shape)  # expected output: 4096, 1\n",
    "print('Output tensor shape for colour: ', colour_predictions.shape)  # expected output: 4096, 3\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in deepsdf.parameters() if p.requires_grad) / 1e6\n",
    "print(f'\\nNumber of traininable params: {num_trainable_params:.2f}M')  # expected output: ~1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesh_to_sdf\n",
    "import trimesh\n",
    "\n",
    "mesh = trimesh.load(r\"E:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\data\\objaverse\\tables\\8aa3d39c9c6b46d4bb39b0fde6204a8a\\mesh.obj\", force=\"mesh\")\n",
    "points, sdf, colours, viewing_directions = mesh_to_sdf.sample_sdf_near_surface(mesh, number_of_points=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:14:25.171606700Z",
     "start_time": "2024-07-23T18:14:18.875051600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "class_label_one_hot: torch.Size([4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml43dg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_deepsdf\n\u001b[0;32m      3\u001b[0m overfit_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0_objaverse_deepsdf_overfit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisualize_every_n\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m     18\u001b[0m }\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain_deepsdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverfit_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\training\\train_deepsdf.py:200\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    197\u001b[0m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml43dg/runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolour_latent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\training\\train_deepsdf.py:84\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, latent_vectors, colour_latent_vectors, train_dataloader, device, config)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_label_one_hot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label_one_hot\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# perform forward pass\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#x_in = torch.cat([batch_latent_vectors, batch_colour_latent_vectors, points, viewing_dirs], dim=1)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m predicted_sdf, predicted_colour \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewing_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_latent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_colour_latent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_label_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# truncate predicted sdf between -0.1 and 0.1\u001b[39;00m\n\u001b[0;32m     86\u001b[0m predicted_sdf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(predicted_sdf, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\ml43dg\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\model\\deepsdf.py:46\u001b[0m, in \u001b[0;36mDeepSDFDecoder.forward\u001b[1;34m(self, point, viewing_direction, shape_latent_code, colour_latent_code, class_label_one_hot)\u001b[0m\n\u001b[0;32m     44\u001b[0m point \u001b[38;5;241m=\u001b[39m point\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     45\u001b[0m viewing_direction \u001b[38;5;241m=\u001b[39m viewing_direction\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 46\u001b[0m x_in \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_latent_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_label_one_hot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwnll1(x_in)))\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwnll2(x)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "from ml43dg.training import train_deepsdf\n",
    "\n",
    "overfit_config = {\n",
    "    'experiment_name': '0_objaverse_deepsdf_overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'num_sample_points': 4096,\n",
    "    'latent_code_length': 256,\n",
    "    'color_latent_code_length': 128,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 5000,\n",
    "    'print_every_n': 150,\n",
    "    'visualize_every_n': 500,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(overfit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the overfitted shape reconstruction to check if it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "\n",
    "from ml43dg.util.visualization import visualize_mesh\n",
    "from ml43dg.data.objaverse import Objaverse\n",
    "\n",
    "# Load and visualize GT mesh of the overfit sample\n",
    "gt_mesh = Objaverse.get_mesh('tables/8aa3d39c9c6b46d4bb39b0fde6204a8a')\n",
    "print('GT')\n",
    "visualize_mesh(gt_mesh.vertices, gt_mesh.faces, flip_axes=True)\n",
    "\n",
    "# Load and visualize reconstructed overfit sample; it's okay if they don't look visually exact, since we don't run\n",
    "# the training too long and have a learning rate decay while training\n",
    "mesh_path = \"ml43dg/runs/0_objaverse_deepsdf_overfit/meshes/02999_000.obj\"\n",
    "overfit_output = trimesh.load(mesh_path)\n",
    "print('Overfit')\n",
    "visualize_mesh(overfit_output.vertices, overfit_output.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mesh = trimesh.load(r\"E:\\ml43dg\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\data\\objaverse\\tables\\8aa3d39c9c6b46d4bb39b0fde6204a8a\\mesh.obj\", force=\"mesh\", process=True)\n",
    "gt_mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over entire train set\n",
    "\n",
    "Once overfitting works, we can train on the entire train set.\n",
    "\n",
    "Note: This training will take a few hours on a GPU (took ~3 hrs for 500 epochs on our 2080Ti, which already gave decent results). Please make sure to start training early enough before the submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Recompute and save the .npz files for all the tables\n",
    "all_tables = list((Path(\"./ml43dg/data\") / \"objaverse\" / \"tables\").iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesh_to_sdf\n",
    "import numpy as np\n",
    "\n",
    "for table_dir in all_tables:\n",
    "    mesh = trimesh.load(table_dir / \"mesh.obj\", force=\"mesh\")\n",
    "    points, sdf, colours, viewing_directions = mesh_to_sdf.sample_sdf_near_surface(mesh, number_of_points=200000)\n",
    "    reshaped = np.column_stack((points, sdf, colours, viewing_directions))\n",
    "    pos = reshaped[reshaped[:, 3] > 0]\n",
    "    neg = reshaped[reshaped[:, 3] < 0]\n",
    "\n",
    "    np.savez(table_dir / \"sdf.npz\", pos=pos, neg=neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-23T18:18:42.406425900Z"
    },
    "is_executing": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ml43dg.training import train_deepsdf\n",
    "\n",
    "generalization_config = {\n",
    "    'experiment_name': '3_1_deepsdf_generalization',\n",
    "    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n",
    "    'is_overfit': False,\n",
    "    'num_sample_points': 4096, # you can adjust this such that the model fits on your gpu\n",
    "    'latent_code_length': 256,\n",
    "    'color_latent_code_length': 128,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 1000, #2000,  # not necessary to run for 2000 epochs if you're short on time, at 500 epochs you should start to see reasonable results\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 1000,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(generalization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model on observed SDF values\n",
    "\n",
    "Fill in the inference script `exercise_3/inference/infer_deepsdf.py`. Note that it's not simply a forward pass, but an optimization of the latent code such that we have lowest error on observed SDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ml43dg.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, 128, \"ml43dg/runs/3_1_deepsdf_generalization\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try inference on a shape from validation set, for which we have a complete observation of sdf values. This is an easier problem as compared to shape completion,\n",
    "since we have all the information already in the input.\n",
    "\n",
    "Let's visualize the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ml43dg.data.objaverse import Objaverse\n",
    "from ml43dg.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "# get observed data\n",
    "points, sdf, colors, viewing_dirs = Objaverse.get_all_sdf_samples(\"tables/3297c67207d64233b53b390062c532d3\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "inside_colors = colors[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "outside_colors = colors[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# convert rgb colors to hex\n",
    "inside_hex_colors = []\n",
    "for color in inside_colors:\n",
    "    inside_hex_colors.append(\"0x{0:02x}{1:02x}{2:02x}\".format(int(color[0]*255), int(color[1]*255), int(color[2]*255)))\n",
    "# convert to np array\n",
    "inside_hex_colors = np.asarray(inside_hex_colors)\n",
    "# convert to int with base 16\n",
    "for i in range(len(inside_hex_colors)):\n",
    "    inside_hex_colors[i] = int(inside_hex_colors[i], 16)\n",
    "    \n",
    "# convert rgb colors to hex\n",
    "outside_hex_colors = []\n",
    "for color in outside_colors:\n",
    "    outside_hex_colors.append(\"0x{0:02x}{1:02x}{2:02x}\".format(int(color[0]*255), int(color[1]*255), int(color[2]*255)))\n",
    "outside_hex_colors = np.asarray(outside_hex_colors)\n",
    "for i in range(len(outside_hex_colors)):\n",
    "    outside_hex_colors[i] = int(outside_hex_colors[i], 16)\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are very complete\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, inside_hex_colors, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, outside_hex_colors, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reconstruction on these observations with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct\n",
    "vertices, faces, vertex_colours = inference_handler.reconstruct(points, sdf, colors, viewing_dirs, 800)\n",
    "\n",
    "# convert vertex_colours to hex values\n",
    "hex_colours = []\n",
    "for color in vertex_colours:\n",
    "    hex_colours.append(\"0x{0:02x}{1:02x}{2:02x}\".format(int(color[0]*255), int(color[1]*255), int(color[2]*255)))\n",
    "hex_colours = np.asarray(hex_colours)\n",
    "for i in range(len(hex_colours)):\n",
    "    hex_colours[i] = int(hex_colours[i], 16)\n",
    "\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, hex_colours, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
