{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\n",
    "\n",
    "**Presentation**: Wednesday, July 24th 16:54\n",
    "\n",
    "**Report and Code**: August 14th, 23:55\n",
    "\n",
    "Dataset:\n",
    "- Objectverse (https://objaverse.allenai.org)\n",
    "\n",
    "Modifications:\n",
    "- Explore various ways of improving the generalization ability across different categories, e.g., adding class embedding, and text feature descriptors.\n",
    "- Adding additional test-time optimization loss for better shape fitting especially for objects with thin structure\n",
    "- Add color code to shape latent code, also learn view-independant color field given colored-mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Running this notebook\n",
    "We recommend running this notebook on a CUDA compatible local gpu. You can also run training on cpu, it will just take longer.\n",
    "\n",
    "You have three options for running this exercise on a GPU, choose one of them and start the exercise below in section \"Imports\":\n",
    "1. Locally on your own GPU\n",
    "2. On our dedicated compute cluster\n",
    "3. On Google Colab\n",
    "\n",
    "We describe every option in more detail below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### (a) Local Execution\n",
    "\n",
    "If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`.\n",
    "\n",
    "In case you are working with a RTX 3000-series GPU, you need to install a patched version of pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Compute Cluster\n",
    "\n",
    "We provide access to a small compute cluster for the exercises and projects, consisting of a login node and 4 compute nodes with one dedicated RTX 3090 GPU each.\n",
    "Please send us a short email with your name and preferred username so we can add you as a user.\n",
    "\n",
    "We uploaded a PDF to Moodle with detailed information on how to access and use the cluster.\n",
    "\n",
    "Since the cluster contains RTX 3000-series GPUs, you will need to install a patched version of pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Google Colab\n",
    "\n",
    "If you don't have access to a GPU and don't want to use our cluster, you can also use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n",
    "What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n",
    "\n",
    "In case you're using Google Colab, you can upload the exercise folder (containing `exercise_2.ipynb`, directory `exercise_2` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n",
    "Additionally you'd need to open the notebook `exercise_2.ipynb` in Colab using `File > Open Notebook > Upload`.\n",
    "\n",
    "Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# We assume you uploaded the exercise folder in root Google Drive folder\n",
    "\n",
    "#!cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\n",
    "#os.chdir('/content/3d-machine-learning/')\n",
    "#print('Installing requirements')\n",
    "#%pip install -r requirements.txt\n",
    "\n",
    "# Make sure you restart runtime when directed by Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell after restarting your colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#import sys\n",
    "#import torch\n",
    "#os.chdir('/content/3d-machine-learning/')\n",
    "#sys.path.insert(1, \"/content/3d-machine-learning/\")\n",
    "#print('CUDA availability:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:04:20.689329400Z",
     "start_time": "2024-07-23T18:04:02.831691900Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet objaverse\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import objaverse\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DeepSDF\n",
    "\n",
    "\n",
    "Here, we will take a look at 3D-reconstruction using [DeepSDF](https://arxiv.org/abs/1901.05103). We recommend reading the paper before attempting the exercise.\n",
    "\n",
    "DeepSDF is an auto-decoder based approach that learns a continuous SDF representation for a class of shapes. Once trained, it can be used for shape representation, interpolation and shape completion. We'll look at each of these\n",
    "applications.\n",
    "\n",
    "<img src=\"ml43dg/images/deepsdf_teaser.png\" alt=\"deepsdf_teaser\" style=\"width: 800px;\"/>\n",
    "\n",
    "During training, the autodecoder optimizes both the network parameters and the latent codes representing each of the training shapes. Once trained, to reconstruct a shape given its SDF observations, a latent code is\n",
    "optimized keeping the network parameters fixed, such that the optimized latent code gives the lowest error with observed SDF values.\n",
    "\n",
    "An advantage that implicit representations have over voxel/grid based approaches is that they are not tied to a particular grid resolution, and can be evaluated at any resolution once trained.\n",
    "\n",
    "Similar to previous exercise, we'll first download the processed dataset, look at the implementation of the dataset, the model and the trainer, try out overfitting and generalization over the entire dataset, and finally inference on unseen samples.\n",
    "\n",
    "### (a) Downloading the data\n",
    "\n",
    "Whereas volumetric models output entire 3d shape representations, implicit models like DeepSDF work on per point basis. The network takes in a 3D-coordinate (and additionally the latent vector) and outputs the SDF value at the queried point. To train such a model,\n",
    "we therefore need, for each of the training shapes, a bunch of points with their corresponding SDF values for supervision. Points are sampled more aggressively near the surface of the object as we want to capture a more detailed SDF near the surface. For those curious,\n",
    "data preparation is decribed in more detail in section 5 of the paper.\n",
    "\n",
    "We'll be using the Objaverse Chairs class for the experiments in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path(\"ml43dg/data/objaverse/raw_chairs\")\n",
    "\n",
    "lvis_annotations = objaverse.load_lvis_annotations()\n",
    "random.seed(42)\n",
    "chairs_uids = lvis_annotations['chair']\n",
    "chairs_uids = random.sample(chairs_uids, 100)\n",
    "\n",
    "objects = objaverse.load_objects(\n",
    "    uids=chairs_uids,\n",
    ")\n",
    "\n",
    "for objaverse_id, file_path in objects.items():\n",
    "    if not Path(download_path / \"chairs\" / objaverse_id).exists():\n",
    "        Path(download_path / \"chairs\").mkdir(parents=True, exist_ok=True)\n",
    "    shutil.move(file_path, download_path / \"chairs\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:57:21.163238400Z",
     "start_time": "2024-07-23T19:57:20.830548100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sofas\n",
      "Downloaded 1 / 80 objects\n",
      "Downloaded 2 / 80 objects\n",
      "Downloaded 3 / 80 objects\n",
      "Downloaded 4 / 80 objects\n",
      "Downloaded 5 / 80 objects\n",
      "Downloaded 6 / 80 objects\n",
      "Downloaded 7 / 80 objects\n",
      "Downloaded 8 / 80 objects\n",
      "Downloaded 9 / 80 objects\n",
      "Downloaded 10 / 80 objects\n",
      "Downloaded 11 / 80 objects\n",
      "Downloaded 12 / 80 objects\n",
      "Downloaded 13 / 80 objects\n",
      "Downloaded 14 / 80 objects\n",
      "Downloaded 15 / 80 objects\n",
      "Downloaded 16 / 80 objects\n",
      "Downloaded 17 / 80 objects\n",
      "Downloaded 18 / 80 objects\n",
      "Downloaded 19 / 80 objects\n",
      "Downloading sofas\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'objaverse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m class_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msofa\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvase\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m (class_name\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m----> 7\u001B[0m     lvis_annotations \u001B[38;5;241m=\u001B[39m objaverse\u001B[38;5;241m.\u001B[39mload_lvis_annotations()\n\u001B[0;32m      8\u001B[0m     random\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m      9\u001B[0m     uids \u001B[38;5;241m=\u001B[39m lvis_annotations[class_name]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'objaverse' is not defined"
     ]
    }
   ],
   "source": [
    "# Downloading sofas, tables, and vases\n",
    "\n",
    "download_path = Path(\"./data/objaverse/\")\n",
    "\n",
    "for class_name in ['sofa', 'table', 'vase']:\n",
    "    print(\"Downloading \" + (class_name+\"s\"))\n",
    "    lvis_annotations = objaverse.load_lvis_annotations()\n",
    "    random.seed(42)\n",
    "    uids = lvis_annotations[class_name]\n",
    "\n",
    "    objects = objaverse.load_objects(\n",
    "        uids=uids,\n",
    "    )\n",
    "\n",
    "    for objaverse_id, file_path in objects.items():\n",
    "        if not Path(download_path / (class_name+\"s\") / objaverse_id).exists():\n",
    "            Path(download_path / (class_name+\"s\")).mkdir(parents=True, exist_ok=True)\n",
    "        shutil.move(file_path, download_path / (class_name+\"s\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shape, the downloaded chair .glb will be converted into a coloured mesh and stored in the same directory with its corresponding sdf file:\n",
    "- `mesh.obj` representing the mesh representation of the shape\n",
    "- `sdf.npz` file containing large number of points sampled on and around the mesh and their sdf values; contains numpy arrays under keys \"pos\" and \"neg\", containing points with positive and negative sdf values respectively\n",
    "\n",
    "```\n",
    "# contents of exercise_3/data/sdf_sofas\n",
    "1faa4c299b93a3e5593ebeeedbff73b/                    # shape 0\n",
    "    ├── mesh.obj                                    # shape 0 mesh\n",
    "    ├── sdf.npz                                     # shape 0 sdf\n",
    "    ├── surface.obj                                 # shape 0 surface\n",
    "1fde48d83065ef5877a929f61fea4d0/                    # shape 1\n",
    "1fe1411b6c8097acf008d8a3590fb522/                   # shape 2\n",
    ":\n",
    "```\n",
    "The processing is performed in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:06:32.655134300Z",
     "start_time": "2024-07-23T18:06:31.716402100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim\\PycharmProjects\\ml43dg-project\\DeepSDF-exercise\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:11:02.328355500Z",
     "start_time": "2024-07-23T18:09:39.261221700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"./ml43dg/data\")\n",
    "\n",
    "# Get the list of files in the sdf_chairs directory\n",
    "sdf_files = list((data_path / \"objaverse\"/ \"sdf_chairs\").iterdir())\n",
    "\n",
    "# For each file in data/objaverse/chairs, create a directory with the same name\n",
    "for file in (data_path / \"objaverse\"/ \"chairs\").iterdir():\n",
    "    if file.is_file():\n",
    "        file_name = file.stem\n",
    "        new_dir = data_path / \"chairs\" / file_name\n",
    "        new_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Then convert the .glb to a .obj mesh file\n",
    "        mesh = trimesh.load(file, force=\"mesh\")\n",
    "        _ = mesh.export(new_dir / f\"{file_name}.obj\")\n",
    "\n",
    "        # Remove the original .glb file\n",
    "        file.unlink()\n",
    "\n",
    "        # Finally, move the corresponding sdf file from data/objaverse/sdf_chairs to the same directory\n",
    "        sdf_file = next(f for f in sdf_files if f.stem == file_name)\n",
    "        shutil.move(sdf_file, new_dir / sdf_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:11:44.231062Z",
     "start_time": "2024-07-23T18:11:43.829958600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename all the sdf files to sdf.npz and all the obj files to mesh.obj\n",
    "for chair_dir in (data_path / \"objaverse\"/ \"chairs\").iterdir():\n",
    "    for file in chair_dir.iterdir():\n",
    "        if file.suffix == \".obj\":\n",
    "            file.rename(chair_dir / \"mesh.obj\")\n",
    "        elif file.suffix == \".npz\":\n",
    "            file.rename(chair_dir / \"sdf.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset\n",
    "\n",
    "We randomly generate train/test splits based on the ratio used in the exercise for ShapeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:12:31.032424Z",
     "start_time": "2024-07-23T18:12:30.950909700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sofas in the ShapeNet training set: 1226\n",
      "Number of sofas in the ShapeNet test set: 137\n",
      "Number of sofas in the ShapeNet overfit set: 1\n",
      "val/train ratio: 10:90\n"
     ]
    }
   ],
   "source": [
    "# Generate random train/test/overfit splits modelled after the split ratios in the ShapeNet case\n",
    "with open(data_path / \"splits\" / \"sofas\" / \"train.txt\", \"r\") as f:\n",
    "    train_sofas = f.read().splitlines()\n",
    "print(f\"Number of sofas in the ShapeNet training set: {len(train_sofas)}\")\n",
    "\n",
    "# Read file splits/sofas/test.txt\n",
    "with open(data_path / \"splits\" / \"sofas\" / \"val.txt\", \"r\") as f:\n",
    "    test_sofas = f.read().splitlines()\n",
    "    print(f\"Number of sofas in the ShapeNet test set: {len(test_sofas)}\")\n",
    "\n",
    "with open(data_path / \"splits\" / \"sofas\" / \"overfit.txt\", \"r\") as f:\n",
    "    overfit_sofas = f.read().splitlines()\n",
    "print(f\"Number of sofas in the ShapeNet overfit set: {len(overfit_sofas)}\")\n",
    "\n",
    "print(f\"val/train ratio: {int(round(len(test_sofas) / (len(train_sofas) + len(test_sofas)),2)*100)}:{int(round(len(train_sofas) / (len(train_sofas) + len(test_sofas)),1)*100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:12:34.217485800Z",
     "start_time": "2024-07-23T18:12:34.142930600Z"
    }
   },
   "outputs": [],
   "source": [
    "# We want to emulate the ratio of the ShapeNet dataset\n",
    "val_ratio = int(round(len(test_sofas) / (len(train_sofas) + len(test_sofas)),2)*100)\n",
    "train_ratio = 100-val_ratio\n",
    "\n",
    "# Get a list of all chairs in the objaverse/chairs directory\n",
    "all_chairs = list((data_path / \"objaverse\"/ \"chairs\").iterdir())\n",
    "\n",
    "# Randomly shuffle the list\n",
    "random.shuffle(all_chairs)\n",
    "\n",
    "# Remove one chair to be used for overfitting\n",
    "overfit_chair = all_chairs.pop()\n",
    "\n",
    "# Split the list into training and validation sets\n",
    "train_chairs = all_chairs[:int(len(all_chairs) * (train_ratio / 100))]\n",
    "val_chairs = all_chairs[int(len(all_chairs) * (train_ratio / 100)):]\n",
    "\n",
    "# Store the names of the chairs in corresponding text files\n",
    "with open(data_path / \"splits\" / \"chairs\" / \"train.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([c.stem for c in train_chairs]))\n",
    "\n",
    "with open(data_path / \"splits\" / \"chairs\" / \"val.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([c.stem for c in val_chairs]))\n",
    "\n",
    "with open(data_path / \"splits\" / \"chairs\" / \"overfit.txt\", \"w\") as f:\n",
    "    f.write(overfit_chair.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:12:52.112225Z",
     "start_time": "2024-07-23T18:12:52.020062300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 406\n",
      "Length of val set: 46\n",
      "Length of overfit set: 1\n"
     ]
    }
   ],
   "source": [
    "from ml43dg.data.objaverse import Objaverse\n",
    "\n",
    "num_points_to_samples = 40000\n",
    "train_dataset = Objaverse(num_points_to_samples, \"train\")\n",
    "val_dataset = Objaverse(num_points_to_samples, \"val\")\n",
    "overfit_dataset = Objaverse(num_points_to_samples, \"overfit\")\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 1226\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the points sampled for a particular shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-23T18:13:07.163868100Z",
     "start_time": "2024-07-23T18:13:07.037204Z"
    }
   },
   "outputs": [],
   "source": [
    "from ml43dg.util.visualization import visualize_mesh, visualize_pointcloud\n",
    "\n",
    "uid = train_dataset[0]['name']\n",
    "points = train_dataset[0]['points']\n",
    "sdf = train_dataset[0]['sdf']\n",
    "\n",
    "# sampled points inside the shape\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "\n",
    "# sampled points outside the shape\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mesh = Objaverse.get_mesh(uid)\n",
    "print('Mesh')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Sampled points with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Sampled points with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that more points are sampled close to the surface rather than away from the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (c) Model\n",
    "\n",
    "The DeepSDF auto-decoder architecture is visualized below:\n",
    "\n",
    "<img src=\"exercise_3/images/deepsdf_architecture.png\" alt=\"deepsdf_arch\" style=\"width: 640px;\"/>\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- The network takes in the latent code for a shape concatenated with the query 3d coordinate, making up a 259 length vector (assuming latent code length is 256).\n",
    "- The network consist of a sequence of weight-normed linear layers, each followed by a ReLU and a dropout. For weight norming a layer, check out `torch.nn.utils.weight_norm`. Each of these linear layers outputs a 512 dimensional vector, except the 4th layer which outputs a 253 dimensional vector.\n",
    "- The output of the 4th layer is concatenated with the input, making the input to the 5th layer a 512 dimensional vector.\n",
    "- The final layer is a simple linear layer without any norm, dropout or non-linearity, with a single dimensional output representing the SDF value.\n",
    "\n",
    "Implement this architecture in file `exercise_3/model/deepsdf.py`.\n",
    "\n",
    "Here are some basic sanity tests once you're done with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-23T18:13:22.232041Z",
     "start_time": "2024-07-23T18:13:22.063142200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name    | Type           | Params \n",
      "---------------------------------------------\n",
      "0  | wnll1   | Linear         | 133632 \n",
      "1  | wnll2   | Linear         | 263168 \n",
      "2  | wnll3   | Linear         | 263168 \n",
      "3  | wnll4   | Linear         | 130042 \n",
      "4  | wnll5   | Linear         | 263168 \n",
      "5  | wnll6   | Linear         | 263168 \n",
      "6  | wnll7   | Linear         | 263168 \n",
      "7  | wnll8   | Linear         | 263168 \n",
      "8  | fc      | Linear         | 513    \n",
      "9  | relu    | ReLU           | 0      \n",
      "10 | dropout | Dropout        | 0      \n",
      "11 | TOTAL   | DeepSDFDecoder | 1843195\n",
      "\n",
      "Output tensor shape:  torch.Size([4096, 1])\n",
      "\n",
      "Number of traininable params: 1.84M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim\\anaconda3\\envs\\ML3D\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from ml43dg.model.deepsdf import DeepSDFDecoder\n",
    "from ml43dg.util.model import summarize_model\n",
    "\n",
    "deepsdf = DeepSDFDecoder(latent_size=256)\n",
    "print(summarize_model(deepsdf))\n",
    "\n",
    "# input to the network is a concatenation of point coordinates (3) and the latent code (256 in this example);\n",
    "# here we use a batch of 4096 points\n",
    "input_tensor = torch.randn(4096, 3 + 256)\n",
    "predictions = deepsdf(input_tensor)\n",
    "\n",
    "print('\\nOutput tensor shape: ', predictions.shape)  # expected output: 4096, 1\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in deepsdf.parameters() if p.requires_grad) / 1e6\n",
    "print(f'\\nNumber of traininable params: {num_trainable_params:.2f}M')  # expected output: ~1.8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training script and overfitting to a single shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T18:14:25.171606700Z",
     "start_time": "2024-07-23T18:14:18.875051600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim\\anaconda3\\envs\\ML3D\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[049/00000] train_loss: 0.011282\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory exercise_3/runs/0_objaverse_deepsdf_overfit does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 19\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mml43dg\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_deepsdf\n\u001B[0;32m      3\u001B[0m overfit_config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexperiment_name\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0_objaverse_deepsdf_overfit\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# change this to cpu if you do not have a GPU\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvisualize_every_n\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m250\u001B[39m,\n\u001B[0;32m     17\u001B[0m }\n\u001B[1;32m---> 19\u001B[0m train_deepsdf\u001B[38;5;241m.\u001B[39mmain(overfit_config)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\training\\train_deepsdf.py:168\u001B[0m, in \u001B[0;36mmain\u001B[1;34m(config)\u001B[0m\n\u001B[0;32m    165\u001B[0m Path(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mml43dg/runs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexperiment_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mmkdir(exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    167\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[1;32m--> 168\u001B[0m train(model, latent_vectors, train_dataloader, device, config)\n",
      "File \u001B[1;32m~\\PycharmProjects\\ml43dg-project\\DeepSDF-exercise\\ml43dg\\training\\train_deepsdf.py:91\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, latent_vectors, train_dataloader, device, config)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;66;03m# save best train model and latent codes\u001B[39;00m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m train_loss \u001B[38;5;241m<\u001B[39m best_loss:\n\u001B[1;32m---> 91\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexercise_3/runs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexperiment_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/model_best.ckpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     92\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(latent_vectors\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexercise_3/runs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexperiment_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/latent_best.ckpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     93\u001B[0m     best_loss \u001B[38;5;241m=\u001B[39m train_loss\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML3D\\Lib\\site-packages\\torch\\serialization.py:627\u001B[0m, in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[0;32m    624\u001B[0m _check_save_filelike(f)\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[1;32m--> 627\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[0;32m    628\u001B[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001B[0;32m    629\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML3D\\Lib\\site-packages\\torch\\serialization.py:501\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[1;34m(name_or_buffer)\u001B[0m\n\u001B[0;32m    499\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    500\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[1;32m--> 501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m container(name_or_buffer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ML3D\\Lib\\site-packages\\torch\\serialization.py:472\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_stream))\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 472\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Parent directory exercise_3/runs/0_objaverse_deepsdf_overfit does not exist."
     ]
    }
   ],
   "source": [
    "from ml43dg.training import train_deepsdf\n",
    "\n",
    "overfit_config = {\n",
    "    'experiment_name': '0_objaverse_deepsdf_overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'num_sample_points': 4096,\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 2000,\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 250,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(overfit_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the overfitted shape reconstruction to check if it looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize GT mesh of the overfit sample\n",
    "gt_mesh = Objaverse.get_mesh('90dfb9e99ddd4b4ca414be7599ea6469')\n",
    "print('GT')\n",
    "visualize_mesh(gt_mesh.vertices, gt_mesh.faces, flip_axes=True)\n",
    "\n",
    "# Load and visualize reconstructed overfit sample; it's okay if they don't look visually exact, since we don't run\n",
    "# the training too long and have a learning rate decay while training\n",
    "mesh_path = \"ml43dg/runs/0_objaverse_deepsdf_overfit/meshes/01999_000.obj\"\n",
    "overfit_output = trimesh.load(mesh_path)\n",
    "print('Overfit')\n",
    "visualize_mesh(overfit_output.vertices, overfit_output.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over entire train set\n",
    "\n",
    "Once overfitting works, we can train on the entire train set.\n",
    "\n",
    "Note: This training will take a few hours on a GPU (took ~3 hrs for 500 epochs on our 2080Ti, which already gave decent results). Please make sure to start training early enough before the submission deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-23T18:18:42.406425900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00049] train_loss: 0.010884\n",
      "[000/00099] train_loss: 0.007273\n",
      "[000/00149] train_loss: 0.006400\n",
      "[000/00199] train_loss: 0.006118\n",
      "[000/00249] train_loss: 0.006073\n",
      "[000/00299] train_loss: 0.006023\n",
      "[000/00349] train_loss: 0.006245\n",
      "[000/00399] train_loss: 0.006030\n",
      "[001/00043] train_loss: 0.006047\n",
      "[001/00093] train_loss: 0.006117\n",
      "[001/00143] train_loss: 0.006130\n",
      "[001/00193] train_loss: 0.006039\n",
      "[001/00243] train_loss: 0.006061\n",
      "[001/00293] train_loss: 0.006095\n",
      "[001/00343] train_loss: 0.006140\n",
      "[001/00393] train_loss: 0.006047\n",
      "[002/00037] train_loss: 0.006084\n",
      "[002/00087] train_loss: 0.006158\n",
      "[002/00137] train_loss: 0.006013\n",
      "[002/00187] train_loss: 0.006048\n",
      "[002/00237] train_loss: 0.006295\n",
      "[002/00287] train_loss: 0.006064\n",
      "[002/00337] train_loss: 0.006048\n",
      "[002/00387] train_loss: 0.006069\n",
      "[003/00031] train_loss: 0.006045\n",
      "[003/00081] train_loss: 0.006075\n",
      "[003/00131] train_loss: 0.006088\n",
      "[003/00181] train_loss: 0.006075\n",
      "[003/00231] train_loss: 0.006064\n",
      "[003/00281] train_loss: 0.006033\n",
      "[003/00331] train_loss: 0.006128\n",
      "[003/00381] train_loss: 0.006059\n",
      "[004/00025] train_loss: 0.005999\n",
      "[004/00075] train_loss: 0.006069\n",
      "[004/00125] train_loss: 0.006086\n",
      "[004/00175] train_loss: 0.006067\n",
      "[004/00225] train_loss: 0.006208\n",
      "[004/00275] train_loss: 0.006027\n",
      "[004/00325] train_loss: 0.006101\n",
      "[004/00375] train_loss: 0.006058\n",
      "[005/00019] train_loss: 0.006174\n",
      "[005/00069] train_loss: 0.006067\n",
      "[005/00119] train_loss: 0.006038\n",
      "[005/00169] train_loss: 0.006105\n",
      "[005/00219] train_loss: 0.006056\n",
      "[005/00269] train_loss: 0.006054\n",
      "[005/00319] train_loss: 0.006159\n",
      "[005/00369] train_loss: 0.006126\n",
      "[006/00013] train_loss: 0.006136\n",
      "[006/00063] train_loss: 0.006116\n",
      "[006/00113] train_loss: 0.006018\n",
      "[006/00163] train_loss: 0.006105\n",
      "[006/00213] train_loss: 0.006101\n",
      "[006/00263] train_loss: 0.006144\n",
      "[006/00313] train_loss: 0.006119\n",
      "[006/00363] train_loss: 0.006086\n",
      "[007/00007] train_loss: 0.006153\n",
      "[007/00057] train_loss: 0.006126\n",
      "[007/00107] train_loss: 0.006064\n",
      "[007/00157] train_loss: 0.006004\n",
      "[007/00207] train_loss: 0.006075\n",
      "[007/00257] train_loss: 0.005994\n",
      "[007/00307] train_loss: 0.006027\n",
      "[007/00357] train_loss: 0.006035\n",
      "[008/00001] train_loss: 0.006148\n",
      "[008/00051] train_loss: 0.006218\n",
      "[008/00101] train_loss: 0.006165\n",
      "[008/00151] train_loss: 0.006127\n",
      "[008/00201] train_loss: 0.006098\n",
      "[008/00251] train_loss: 0.006042\n",
      "[008/00301] train_loss: 0.006063\n",
      "[008/00351] train_loss: 0.006053\n",
      "[008/00401] train_loss: 0.005974\n",
      "[009/00045] train_loss: 0.006137\n",
      "[009/00095] train_loss: 0.006055\n",
      "[009/00145] train_loss: 0.006102\n",
      "[009/00195] train_loss: 0.006108\n",
      "[009/00245] train_loss: 0.006065\n",
      "[009/00295] train_loss: 0.006027\n",
      "[009/00345] train_loss: 0.006199\n",
      "[009/00395] train_loss: 0.006162\n",
      "[010/00039] train_loss: 0.006065\n",
      "[010/00089] train_loss: 0.006124\n",
      "[010/00139] train_loss: 0.006075\n",
      "[010/00189] train_loss: 0.006109\n",
      "[010/00239] train_loss: 0.006096\n",
      "[010/00289] train_loss: 0.006018\n",
      "[010/00339] train_loss: 0.006163\n",
      "[010/00389] train_loss: 0.006085\n",
      "[011/00033] train_loss: 0.006119\n",
      "[011/00083] train_loss: 0.006154\n",
      "[011/00133] train_loss: 0.006120\n",
      "[011/00183] train_loss: 0.005968\n",
      "[011/00233] train_loss: 0.006034\n",
      "[011/00283] train_loss: 0.006104\n",
      "[011/00333] train_loss: 0.006160\n",
      "[011/00383] train_loss: 0.006100\n",
      "[012/00027] train_loss: 0.006075\n",
      "[012/00077] train_loss: 0.006106\n",
      "[012/00127] train_loss: 0.005993\n",
      "[012/00177] train_loss: 0.006127\n",
      "[012/00227] train_loss: 0.006055\n",
      "[012/00277] train_loss: 0.006041\n",
      "[012/00327] train_loss: 0.006082\n",
      "[012/00377] train_loss: 0.006128\n",
      "[013/00021] train_loss: 0.006104\n",
      "[013/00071] train_loss: 0.006042\n",
      "[013/00121] train_loss: 0.006099\n",
      "[013/00171] train_loss: 0.006060\n",
      "[013/00221] train_loss: 0.006021\n",
      "[013/00271] train_loss: 0.006155\n",
      "[013/00321] train_loss: 0.006068\n",
      "[013/00371] train_loss: 0.006150\n",
      "[014/00015] train_loss: 0.006095\n",
      "[014/00065] train_loss: 0.006047\n",
      "[014/00115] train_loss: 0.006033\n",
      "[014/00165] train_loss: 0.005988\n",
      "[014/00215] train_loss: 0.006118\n",
      "[014/00265] train_loss: 0.006094\n",
      "[014/00315] train_loss: 0.006012\n",
      "[014/00365] train_loss: 0.006006\n"
     ]
    }
   ],
   "source": [
    "from ml43dg.training import train_deepsdf\n",
    "\n",
    "generalization_config = {\n",
    "    'experiment_name': '3_1_deepsdf_generalization',\n",
    "    'device': 'cuda:0',  # run this on a gpu for a reasonable training time\n",
    "    'is_overfit': False,\n",
    "    'num_sample_points': 4096, # you can adjust this such that the model fits on your gpu\n",
    "    'latent_code_length': 256,\n",
    "    'batch_size': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.0005,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'lambda_code_regularization': 0.0001,\n",
    "    'max_epochs': 1000, #2000,  # not necessary to run for 2000 epochs if you're short on time, at 500 epochs you should start to see reasonable results\n",
    "    'print_every_n': 50,\n",
    "    'visualize_every_n': 5000,\n",
    "}\n",
    "\n",
    "train_deepsdf.main(generalization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model on observed SDF values\n",
    "\n",
    "Fill in the inference script `exercise_3/inference/infer_deepsdf.py`. Note that it's not simply a forward pass, but an optimization of the latent code such that we have lowest error on observed SDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ml43dg.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "device = torch.device('cuda:0')  # change this to cpu if you're not using a gpu\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_1_deepsdf_generalization\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try inference on a shape from validation set, for which we have a complete observation of sdf values. This is an easier problem as compared to shape completion,\n",
    "since we have all the information already in the input.\n",
    "\n",
    "Let's visualize the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get observed data\n",
    "points, sdf = Objaverse.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are very complete\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reconstruction on these observations with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try the shape completion task, i.e., inference on a shape from validation set, for which we do not have a complete observation of sdf values. The observed points are visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get observed data\n",
    "points, sdf = Objaverse.get_all_sdf_samples(\"b351e06f5826444c19fb4103277a6b93_incomplete\")\n",
    "\n",
    "inside_points = points[sdf[:, 0] < 0, :].numpy()\n",
    "outside_points = points[sdf[:, 0] > 0, :].numpy()\n",
    "\n",
    "# visualize observed points; you'll observe that the observations are incomplete\n",
    "# making this is a shape completion task\n",
    "print('Observations with negative SDF (inside)')\n",
    "visualize_pointcloud(inside_points, 0.025, flip_axes=True)\n",
    "print('Observations with positive SDF (outside)')\n",
    "visualize_pointcloud(outside_points, 0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape completion using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct\n",
    "vertices, faces = inference_handler.reconstruct(points, sdf, 800)\n",
    "# visualize\n",
    "visualize_mesh(vertices, faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Latent space interpolation\n",
    "\n",
    "The latent space learned by DeepSDF is interpolatable, meaning that decoding latent codes from this space produced meaningful shapes. Given two latent codes, a linearly interpolatable latent space will decode\n",
    "each of the intermediate codes to some valid shape. Let's see if this holds for our trained model.\n",
    "\n",
    "We'll pick two shapes from the train set as visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml43dg.data.shape_implicit import ShapeImplicit\n",
    "from ml43dg.util.visualization import visualize_mesh\n",
    "\n",
    "mesh = Objaverse.get_mesh(\"494fe53da65650b8c358765b76c296\")\n",
    "print('GT Shape A')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)\n",
    "\n",
    "mesh = Objaverse.get_mesh(\"5ca1ef55ff5f68501921e7a85cf9da35\")\n",
    "print('GT Shape B')\n",
    "visualize_mesh(mesh.vertices, mesh.faces, flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement the missing parts in `exercise_3/inference/infer_deepsdf.py` such that it interpolates two given latent vectors, and run the code fragement below once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ml43dg.inference.infer_deepsdf import InferenceHandlerDeepSDF\n",
    "\n",
    "inference_handler = InferenceHandlerDeepSDF(256, \"exercise_3/runs/3_1_deepsdf_generalization\", torch.device('cuda:0'))\n",
    "# interpolate; also exports interpolated meshes to disk\n",
    "inference_handler.interpolate('494fe53da65650b8c358765b76c296', '5ca1ef55ff5f68501921e7a85cf9da35', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the interpolation below. If everything works out correctly, you should see a smooth transformation between the shapes, with all intermediate shapes being valid sofas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ml43dg.util.mesh_collection_to_gif import  meshes_to_gif\n",
    "from ml43dg.util.misc import show_gif\n",
    "\n",
    "# create list of meshes (just exported) to be visualized\n",
    "mesh_paths = sorted([x for x in Path(\"exercise_3/runs/3_1_deepsdf_generalization/interpolation\").iterdir() if int(x.name.split('.')[0].split(\"_\")[1]) == 0], key=lambda x: int(x.name.split('.')[0].split(\"_\")[0]))\n",
    "mesh_paths = mesh_paths + mesh_paths[::-1]\n",
    "\n",
    "# create a visualization of the interpolation process\n",
    "meshes_to_gif(mesh_paths, \"exercise_3/runs/3_1_deepsdf_generalization/latent_interp.gif\", 20)\n",
    "show_gif(\"exercise_3/runs/3_1_deepsdf_generalization/latent_interp.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 3 🙂. Please create a zip containing all files we provided, everything you modified, your visualization images/gif (no need to submit generated OBJs), including your checkpoints. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle.\n",
    "\n",
    "**Note**: The maximum submission file size limit for Moodle is 100M. You do not need to submit your overfitting checkpoints; however, the generalization checkpoint will be >200M. The easiest way to still be able to submit that one is to split it with zip like this: `zip -s 100M model_best.ckpt.zip model_best.ckpt` which creates a `.zip` and a `.z01`. You can then submit both files alongside another zip containing all your code and outputs.\n",
    "\n",
    "**Submission Deadline**: 11.06.2024, 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
